#!/usr/bin/env python3
"""
æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
APIæƒé™é‡æ„é¡¹ç›® - ä»»åŠ¡3.5
åˆ›å»ºæ—¶é—´: 2025-01-10
"""

import asyncio
import asyncpg
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class OptimizationRecommendation:
    """ä¼˜åŒ–å»ºè®®"""
    category: str
    priority: str  # 'high', 'medium', 'low'
    title: str
    description: str
    impact: str
    implementation_effort: str
    sql_commands: List[str]

class DatabasePerformanceReportGenerator:
    """æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.conn: Optional[asyncpg.Connection] = None
        self.report_data = {}
        self.recommendations = []
        
        # æƒé™ç›¸å…³è¡¨åˆ—è¡¨
        self.permission_tables = [
            't_sys_api_endpoints',
            't_sys_user_permissions', 
            't_sys_role_permissions',
            't_sys_user_roles'
        ]
    
    async def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = await asyncpg.connect(self.database_url)
            logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    async def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.conn:
            await self.conn.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²æ–­å¼€")
    
    async def generate_comprehensive_report(self, output_dir: str = "reports") -> str:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("å¼€å§‹ç”Ÿæˆæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # æ”¶é›†å„ç§æ€§èƒ½æ•°æ®
        await self._collect_table_statistics()
        await self._collect_index_analysis()
        await self._collect_query_performance()
        await self._collect_cache_statistics()
        await self._collect_connection_analysis()
        await self._analyze_optimization_opportunities()
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        report_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_report_path = output_path / f"performance_optimization_report_{report_timestamp}.md"
        await self._generate_markdown_report(md_report_path)
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        json_report_path = output_path / f"performance_data_{report_timestamp}.json"
        await self._generate_json_report(json_report_path)
        
        # ç”Ÿæˆå›¾è¡¨
        charts_dir = output_path / f"charts_{report_timestamp}"
        await self._generate_performance_charts(charts_dir)
        
        logger.info(f"æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {md_report_path}")
        return str(md_report_path)
    
    async def _collect_table_statistics(self):
        """æ”¶é›†è¡¨ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("æ”¶é›†è¡¨ç»Ÿè®¡ä¿¡æ¯...")
        
        query = """
            SELECT 
                schemaname,
                tablename,
                seq_scan,
                seq_tup_read,
                idx_scan,
                idx_tup_fetch,
                n_tup_ins,
                n_tup_upd,
                n_tup_del,
                n_tup_hot_upd,
                n_live_tup,
                n_dead_tup,
                last_vacuum,
                last_autovacuum,
                last_analyze,
                last_autoanalyze,
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as table_size
            FROM pg_stat_user_tables 
            WHERE tablename = ANY($1)
            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
        """
        
        rows = await self.conn.fetch(query, self.permission_tables)
        
        table_stats = []
        for row in rows:
            total_scans = (row['seq_scan'] or 0) + (row['idx_scan'] or 0)
            seq_scan_ratio = (row['seq_scan'] or 0) / max(total_scans, 1) if total_scans > 0 else 0
            
            # è®¡ç®—æ­»å…ƒç»„æ¯”ä¾‹
            total_tuples = (row['n_live_tup'] or 0) + (row['n_dead_tup'] or 0)
            dead_tuple_ratio = (row['n_dead_tup'] or 0) / max(total_tuples, 1) if total_tuples > 0 else 0
            
            stats = dict(row)
            stats.update({
                'total_scans': total_scans,
                'seq_scan_ratio': seq_scan_ratio,
                'dead_tuple_ratio': dead_tuple_ratio,
                'needs_vacuum': dead_tuple_ratio > 0.1,
                'high_seq_scan': seq_scan_ratio > 0.5 and total_scans > 100
            })
            table_stats.append(stats)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            if stats['high_seq_scan']:
                self.recommendations.append(OptimizationRecommendation(
                    category='ç´¢å¼•ä¼˜åŒ–',
                    priority='high',
                    title=f'è¡¨ {row["tablename"]} é¡ºåºæ‰«æè¿‡å¤š',
                    description=f'è¯¥è¡¨çš„é¡ºåºæ‰«ææ¯”ä¾‹ä¸º {seq_scan_ratio:.1%}ï¼Œå»ºè®®æ·»åŠ é€‚å½“çš„ç´¢å¼•',
                    impact='æ˜¾è‘—æå‡æŸ¥è¯¢æ€§èƒ½',
                    implementation_effort='ä¸­ç­‰',
                    sql_commands=[
                        f"-- åˆ†æè¡¨ {row['tablename']} çš„æŸ¥è¯¢æ¨¡å¼",
                        f"ANALYZE {row['tablename']};",
                        f"-- æ ¹æ®æŸ¥è¯¢æ¨¡å¼æ·»åŠ é€‚å½“çš„ç´¢å¼•"
                    ]
                ))
            
            if stats['needs_vacuum']:
                self.recommendations.append(OptimizationRecommendation(
                    category='ç»´æŠ¤ä¼˜åŒ–',
                    priority='medium',
                    title=f'è¡¨ {row["tablename"]} éœ€è¦æ¸…ç†',
                    description=f'è¯¥è¡¨çš„æ­»å…ƒç»„æ¯”ä¾‹ä¸º {dead_tuple_ratio:.1%}ï¼Œå»ºè®®æ‰§è¡ŒVACUUM',
                    impact='é‡Šæ”¾å­˜å‚¨ç©ºé—´ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½',
                    implementation_effort='ä½',
                    sql_commands=[
                        f"VACUUM ANALYZE {row['tablename']};",
                        f"-- è€ƒè™‘è°ƒæ•´autovacuumå‚æ•°"
                    ]
                ))
        
        self.report_data['table_statistics'] = table_stats
    
    async def _collect_index_analysis(self):
        """æ”¶é›†ç´¢å¼•åˆ†æä¿¡æ¯"""
        logger.info("æ”¶é›†ç´¢å¼•åˆ†æä¿¡æ¯...")
        
        # ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡
        index_usage_query = """
            SELECT 
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch,
                pg_size_pretty(pg_relation_size(indexname::regclass)) as index_size,
                pg_relation_size(indexname::regclass) as index_size_bytes
            FROM pg_stat_user_indexes 
            WHERE tablename = ANY($1)
            ORDER BY idx_scan DESC
        """
        
        index_rows = await self.conn.fetch(index_usage_query, self.permission_tables)
        
        # ç´¢å¼•å®šä¹‰æŸ¥è¯¢
        index_def_query = """
            SELECT 
                schemaname,
                tablename,
                indexname,
                indexdef
            FROM pg_indexes 
            WHERE tablename = ANY($1)
            ORDER BY tablename, indexname
        """
        
        index_def_rows = await self.conn.fetch(index_def_query, self.permission_tables)
        
        # åˆå¹¶ç´¢å¼•ä¿¡æ¯
        index_analysis = []
        for usage_row in index_rows:
            # æŸ¥æ‰¾å¯¹åº”çš„ç´¢å¼•å®šä¹‰
            index_def = next(
                (def_row['indexdef'] for def_row in index_def_rows 
                 if def_row['indexname'] == usage_row['indexname']),
                'Unknown'
            )
            
            analysis = dict(usage_row)
            analysis['indexdef'] = index_def
            analysis['is_unused'] = (usage_row['idx_scan'] or 0) < 10
            analysis['efficiency'] = (usage_row['idx_tup_fetch'] or 0) / max(usage_row['idx_tup_read'] or 1, 1)
            
            index_analysis.append(analysis)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            if analysis['is_unused'] and not usage_row['indexname'].endswith('_pkey'):
                self.recommendations.append(OptimizationRecommendation(
                    category='ç´¢å¼•ä¼˜åŒ–',
                    priority='low',
                    title=f'ç´¢å¼• {usage_row["indexname"]} ä½¿ç”¨ç‡ä½',
                    description=f'è¯¥ç´¢å¼•æ‰«ææ¬¡æ•°ä»…ä¸º {usage_row["idx_scan"]}ï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦åˆ é™¤',
                    impact='èŠ‚çœå­˜å‚¨ç©ºé—´ï¼Œæå‡å†™å…¥æ€§èƒ½',
                    implementation_effort='ä½',
                    sql_commands=[
                        f"-- ç¡®è®¤ç´¢å¼•ä¸å†éœ€è¦ååˆ é™¤",
                        f"DROP INDEX IF EXISTS {usage_row['indexname']};"
                    ]
                ))
        
        self.report_data['index_analysis'] = index_analysis
        
        # ç¼ºå¤±ç´¢å¼•åˆ†æ
        await self._analyze_missing_indexes()
    
    async def _analyze_missing_indexes(self):
        """åˆ†æå¯èƒ½ç¼ºå¤±çš„ç´¢å¼•"""
        logger.info("åˆ†æå¯èƒ½ç¼ºå¤±çš„ç´¢å¼•...")
        
        missing_indexes = []
        
        # æ£€æŸ¥æƒé™éªŒè¯ç›¸å…³çš„å¸¸ç”¨æŸ¥è¯¢æ¨¡å¼
        common_patterns = [
            {
                'table': 't_sys_user_permissions',
                'columns': ['user_id', 'permission_code', 'is_active'],
                'reason': 'ç”¨æˆ·æƒé™éªŒè¯æŸ¥è¯¢'
            },
            {
                'table': 't_sys_role_permissions', 
                'columns': ['role_id', 'permission_code', 'is_active'],
                'reason': 'è§’è‰²æƒé™éªŒè¯æŸ¥è¯¢'
            },
            {
                'table': 't_sys_api_endpoints',
                'columns': ['api_path', 'http_method', 'status'],
                'reason': 'APIæƒé™éªŒè¯æŸ¥è¯¢'
            }
        ]
        
        for pattern in common_patterns:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„å¤åˆç´¢å¼•
            index_check_query = """
                SELECT COUNT(*) as index_count
                FROM pg_indexes 
                WHERE tablename = $1 
                  AND indexdef ILIKE $2
            """
            
            index_pattern = f"%{', '.join(pattern['columns'])}%"
            count = await self.conn.fetchval(index_check_query, pattern['table'], index_pattern)
            
            if count == 0:
                missing_indexes.append({
                    'table': pattern['table'],
                    'columns': pattern['columns'],
                    'reason': pattern['reason'],
                    'suggested_name': f"idx_{pattern['table'].replace('t_sys_', '')}_{('_'.join(pattern['columns']))}"
                })
                
                # ç”Ÿæˆä¼˜åŒ–å»ºè®®
                self.recommendations.append(OptimizationRecommendation(
                    category='ç´¢å¼•ä¼˜åŒ–',
                    priority='high',
                    title=f'å»ºè®®ä¸º {pattern["table"]} æ·»åŠ å¤åˆç´¢å¼•',
                    description=f'ä¸º {pattern["reason"]} æ·»åŠ å¤åˆç´¢å¼•å¯æ˜¾è‘—æå‡æ€§èƒ½',
                    impact='æ˜¾è‘—æå‡æŸ¥è¯¢æ€§èƒ½',
                    implementation_effort='ä½',
                    sql_commands=[
                        f"CREATE INDEX CONCURRENTLY {missing_indexes[-1]['suggested_name']}",
                        f"ON {pattern['table']}({', '.join(pattern['columns'])});",
                        f"-- ç”¨äºä¼˜åŒ–: {pattern['reason']}"
                    ]
                ))
        
        self.report_data['missing_indexes'] = missing_indexes
    
    async def _collect_query_performance(self):
        """æ”¶é›†æŸ¥è¯¢æ€§èƒ½ä¿¡æ¯"""
        logger.info("æ”¶é›†æŸ¥è¯¢æ€§èƒ½ä¿¡æ¯...")
        
        # æ£€æŸ¥pg_stat_statementsæ‰©å±•
        extension_check = await self.conn.fetchval("""
            SELECT EXISTS (
                SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'
            )
        """)
        
        if not extension_check:
            logger.warning("pg_stat_statementsæ‰©å±•æœªå®‰è£…ï¼Œè·³è¿‡æŸ¥è¯¢æ€§èƒ½åˆ†æ")
            self.report_data['query_performance'] = {
                'available': False,
                'message': 'pg_stat_statementsæ‰©å±•æœªå®‰è£…'
            }
            return
        
        # æŸ¥è¯¢æ…¢æŸ¥è¯¢ç»Ÿè®¡
        slow_queries_query = """
            SELECT 
                query,
                calls,
                total_time,
                mean_time,
                max_time,
                min_time,
                rows,
                shared_blks_hit,
                shared_blks_read,
                shared_blks_dirtied,
                shared_blks_written,
                100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
            FROM pg_stat_statements 
            WHERE query ILIKE ANY(ARRAY['%t_sys_%', '%permission%', '%api%'])
            ORDER BY mean_time DESC
            LIMIT 20
        """
        
        slow_queries = await self.conn.fetch(slow_queries_query)
        
        query_stats = []
        for row in slow_queries:
            stats = dict(row)
            stats['is_slow'] = row['mean_time'] > 100  # 100msé˜ˆå€¼
            stats['cache_efficiency'] = row['hit_percent'] or 0
            query_stats.append(stats)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            if stats['is_slow']:
                self.recommendations.append(OptimizationRecommendation(
                    category='æŸ¥è¯¢ä¼˜åŒ–',
                    priority='high',
                    title=f'æ…¢æŸ¥è¯¢ä¼˜åŒ– (å¹³å‡ {row["mean_time"]:.1f}ms)',
                    description=f'è¯¥æŸ¥è¯¢å¹³å‡æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œéœ€è¦ä¼˜åŒ–',
                    impact='æ˜¾è‘—æå‡å“åº”æ—¶é—´',
                    implementation_effort='ä¸­ç­‰åˆ°é«˜',
                    sql_commands=[
                        "-- ä½¿ç”¨EXPLAIN ANALYZEåˆ†ææŸ¥è¯¢è®¡åˆ’",
                        f"EXPLAIN (ANALYZE, BUFFERS) {row['query'][:100]}...;",
                        "-- æ ¹æ®æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–ç´¢å¼•å’ŒæŸ¥è¯¢ç»“æ„"
                    ]
                ))
        
        self.report_data['query_performance'] = {
            'available': True,
            'slow_queries': query_stats,
            'total_queries_analyzed': len(query_stats)
        }
    
    async def _collect_cache_statistics(self):
        """æ”¶é›†ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("æ”¶é›†ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")
        
        # æ•°æ®åº“çº§åˆ«ç¼“å­˜ç»Ÿè®¡
        db_cache_query = """
            SELECT 
                sum(heap_blks_read) as heap_read,
                sum(heap_blks_hit) as heap_hit,
                sum(idx_blks_read) as idx_read,
                sum(idx_blks_hit) as idx_hit,
                sum(toast_blks_read) as toast_read,
                sum(toast_blks_hit) as toast_hit
            FROM pg_statio_user_tables 
            WHERE relname = ANY($1)
        """
        
        cache_row = await self.conn.fetchrow(db_cache_query, self.permission_tables)
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_heap = (cache_row['heap_read'] or 0) + (cache_row['heap_hit'] or 0)
        total_idx = (cache_row['idx_read'] or 0) + (cache_row['idx_hit'] or 0)
        
        heap_hit_ratio = (cache_row['heap_hit'] or 0) / max(total_heap, 1) if total_heap > 0 else 0
        idx_hit_ratio = (cache_row['idx_hit'] or 0) / max(total_idx, 1) if total_idx > 0 else 0
        
        # ç³»ç»Ÿçº§åˆ«ç¼“å­˜ç»Ÿè®¡
        system_cache_query = """
            SELECT 
                setting as shared_buffers
            FROM pg_settings 
            WHERE name = 'shared_buffers'
        """
        
        shared_buffers = await self.conn.fetchval(system_cache_query)
        
        cache_stats = {
            'heap_hit_ratio': heap_hit_ratio,
            'index_hit_ratio': idx_hit_ratio,
            'shared_buffers': shared_buffers,
            'total_heap_blocks': total_heap,
            'total_index_blocks': total_idx,
            'cache_efficiency': 'good' if heap_hit_ratio > 0.95 else 'needs_improvement'
        }
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if heap_hit_ratio < 0.95 and total_heap > 1000:
            self.recommendations.append(OptimizationRecommendation(
                category='ç¼“å­˜ä¼˜åŒ–',
                priority='medium',
                title='æ•°æ®åº“ç¼“å­˜å‘½ä¸­ç‡åä½',
                description=f'å †ç¼“å­˜å‘½ä¸­ç‡ä¸º {heap_hit_ratio:.1%}ï¼Œå»ºè®®è°ƒæ•´shared_bufferså‚æ•°',
                impact='æå‡æ•´ä½“æŸ¥è¯¢æ€§èƒ½',
                implementation_effort='ä¸­ç­‰',
                sql_commands=[
                    "-- åœ¨postgresql.confä¸­è°ƒæ•´shared_buffers",
                    "# shared_buffers = 256MB  # å»ºè®®è®¾ç½®ä¸ºå†…å­˜çš„25%",
                    "-- é‡å¯æ•°æ®åº“æœåŠ¡ä½¿é…ç½®ç”Ÿæ•ˆ"
                ]
            ))
        
        self.report_data['cache_statistics'] = cache_stats
    
    async def _collect_connection_analysis(self):
        """æ”¶é›†è¿æ¥åˆ†æä¿¡æ¯"""
        logger.info("æ”¶é›†è¿æ¥åˆ†æä¿¡æ¯...")
        
        connection_query = """
            SELECT 
                count(*) as total_connections,
                count(*) FILTER (WHERE state = 'active') as active_connections,
                count(*) FILTER (WHERE state = 'idle') as idle_connections,
                count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction,
                max(setting::int) as max_connections,
                avg(EXTRACT(EPOCH FROM (now() - query_start))) as avg_query_duration
            FROM pg_stat_activity, pg_settings 
            WHERE pg_settings.name = 'max_connections'
              AND pid != pg_backend_pid()
        """
        
        conn_row = await self.conn.fetchrow(connection_query)
        
        connection_usage = conn_row['total_connections'] / max(conn_row['max_connections'], 1)
        
        connection_stats = dict(conn_row)
        connection_stats.update({
            'connection_usage_ratio': connection_usage,
            'connection_efficiency': 'good' if connection_usage < 0.8 else 'high_usage'
        })
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if connection_usage > 0.8:
            self.recommendations.append(OptimizationRecommendation(
                category='è¿æ¥ä¼˜åŒ–',
                priority='high',
                title='æ•°æ®åº“è¿æ¥ä½¿ç”¨ç‡è¿‡é«˜',
                description=f'è¿æ¥ä½¿ç”¨ç‡ä¸º {connection_usage:.1%}ï¼Œå»ºè®®ä½¿ç”¨è¿æ¥æ± æˆ–å¢åŠ æœ€å¤§è¿æ¥æ•°',
                impact='é¿å…è¿æ¥è€—å°½ï¼Œæå‡ç³»ç»Ÿç¨³å®šæ€§',
                implementation_effort='ä¸­ç­‰',
                sql_commands=[
                    "-- åœ¨postgresql.confä¸­è°ƒæ•´max_connections",
                    "# max_connections = 200",
                    "-- æˆ–è€…ä½¿ç”¨è¿æ¥æ± å¦‚PgBouncer"
                ]
            ))
        
        if conn_row['idle_in_transaction'] > 5:
            self.recommendations.append(OptimizationRecommendation(
                category='è¿æ¥ä¼˜åŒ–',
                priority='medium',
                title='å­˜åœ¨è¿‡å¤šç©ºé—²äº‹åŠ¡è¿æ¥',
                description=f'æœ‰ {conn_row["idle_in_transaction"]} ä¸ªç©ºé—²äº‹åŠ¡è¿æ¥ï¼Œå¯èƒ½å¯¼è‡´é”ç­‰å¾…',
                impact='å‡å°‘é”ç­‰å¾…ï¼Œæå‡å¹¶å‘æ€§èƒ½',
                implementation_effort='ä½',
                sql_commands=[
                    "-- è®¾ç½®ç©ºé—²äº‹åŠ¡è¶…æ—¶",
                    "# idle_in_transaction_session_timeout = 60s"
                ]
            ))
        
        self.report_data['connection_analysis'] = connection_stats
    
    async def _analyze_optimization_opportunities(self):
        """åˆ†æä¼˜åŒ–æœºä¼š"""
        logger.info("åˆ†æä¼˜åŒ–æœºä¼š...")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºå»ºè®®
        self.recommendations.sort(key=lambda x: {'high': 3, 'medium': 2, 'low': 1}[x.priority], reverse=True)
        
        # ç»Ÿè®¡å»ºè®®åˆ†ç±»
        recommendation_summary = {
            'total_recommendations': len(self.recommendations),
            'by_priority': {
                'high': len([r for r in self.recommendations if r.priority == 'high']),
                'medium': len([r for r in self.recommendations if r.priority == 'medium']),
                'low': len([r for r in self.recommendations if r.priority == 'low'])
            },
            'by_category': {}
        }
        
        for rec in self.recommendations:
            if rec.category not in recommendation_summary['by_category']:
                recommendation_summary['by_category'][rec.category] = 0
            recommendation_summary['by_category'][rec.category] += 1
        
        self.report_data['optimization_summary'] = recommendation_summary
    
    async def _generate_markdown_report(self, output_path: Path):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        logger.info(f"ç”ŸæˆMarkdownæŠ¥å‘Š: {output_path}")
        
        report_content = f"""# æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ•°æ®åº“**: PostgreSQL  
**åˆ†æèŒƒå›´**: APIæƒé™ç³»ç»Ÿç›¸å…³è¡¨  

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šåˆ†æäº†APIæƒé™ç³»ç»Ÿç›¸å…³æ•°æ®åº“è¡¨çš„æ€§èƒ½çŠ¶å†µï¼Œè¯†åˆ«äº† **{self.report_data['optimization_summary']['total_recommendations']}** ä¸ªä¼˜åŒ–æœºä¼šï¼Œå…¶ä¸­ï¼š

- ğŸ”´ é«˜ä¼˜å…ˆçº§: {self.report_data['optimization_summary']['by_priority']['high']} ä¸ª
- ğŸŸ¡ ä¸­ä¼˜å…ˆçº§: {self.report_data['optimization_summary']['by_priority']['medium']} ä¸ª  
- ğŸŸ¢ ä½ä¼˜å…ˆçº§: {self.report_data['optimization_summary']['by_priority']['low']} ä¸ª

## 1. è¡¨ç»Ÿè®¡åˆ†æ

### 1.1 è¡¨å¤§å°å’Œè®¿é—®æ¨¡å¼

| è¡¨å | å¤§å° | æ´»è·ƒå…ƒç»„ | æ­»å…ƒç»„ | é¡ºåºæ‰«ææ¯”ä¾‹ | çŠ¶æ€ |
|------|------|----------|--------|--------------|------|
"""
        
        for table in self.report_data['table_statistics']:
            status = "âš ï¸ éœ€è¦ä¼˜åŒ–" if table['high_seq_scan'] or table['needs_vacuum'] else "âœ… æ­£å¸¸"
            report_content += f"| {table['tablename']} | {table['table_size']} | {table['n_live_tup']:,} | {table['n_dead_tup']:,} | {table['seq_scan_ratio']:.1%} | {status} |\n"
        
        report_content += f"""

### 1.2 è¡¨ç»´æŠ¤çŠ¶æ€

"""
        
        for table in self.report_data['table_statistics']:
            if table['needs_vacuum'] or table['high_seq_scan']:
                report_content += f"""
**{table['tablename']}**:
- æ­»å…ƒç»„æ¯”ä¾‹: {table['dead_tuple_ratio']:.1%}
- é¡ºåºæ‰«ææ¯”ä¾‹: {table['seq_scan_ratio']:.1%}
- æœ€åVACUUM: {table['last_vacuum'] or 'æœªçŸ¥'}
- æœ€åANALYZE: {table['last_analyze'] or 'æœªçŸ¥'}
"""

        report_content += f"""

## 2. ç´¢å¼•åˆ†æ

### 2.1 ç°æœ‰ç´¢å¼•ä½¿ç”¨æƒ…å†µ

| ç´¢å¼•å | è¡¨å | æ‰«ææ¬¡æ•° | å¤§å° | æ•ˆç‡ | çŠ¶æ€ |
|--------|------|----------|------|------|------|
"""
        
        for index in self.report_data['index_analysis']:
            status = "âš ï¸ ä½ä½¿ç”¨ç‡" if index['is_unused'] else "âœ… æ­£å¸¸ä½¿ç”¨"
            report_content += f"| {index['indexname']} | {index['tablename']} | {index['idx_scan']:,} | {index['index_size']} | {index['efficiency']:.2f} | {status} |\n"
        
        if self.report_data.get('missing_indexes'):
            report_content += f"""

### 2.2 å»ºè®®æ·»åŠ çš„ç´¢å¼•

"""
            for missing in self.report_data['missing_indexes']:
                report_content += f"""
**{missing['suggested_name']}**:
- è¡¨: {missing['table']}
- åˆ—: {', '.join(missing['columns'])}
- ç”¨é€”: {missing['reason']}
"""

        # æŸ¥è¯¢æ€§èƒ½åˆ†æ
        if self.report_data['query_performance']['available']:
            report_content += f"""

## 3. æŸ¥è¯¢æ€§èƒ½åˆ†æ

### 3.1 æ…¢æŸ¥è¯¢ç»Ÿè®¡

| å¹³å‡æ—¶é—´(ms) | è°ƒç”¨æ¬¡æ•° | ç¼“å­˜å‘½ä¸­ç‡ | æŸ¥è¯¢ç‰‡æ®µ |
|--------------|----------|------------|----------|
"""
            
            for query in self.report_data['query_performance']['slow_queries'][:10]:
                query_snippet = query['query'][:80].replace('\n', ' ') + '...'
                report_content += f"| {query['mean_time']:.1f} | {query['calls']:,} | {query['cache_efficiency']:.1f}% | `{query_snippet}` |\n"
        
        # ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.report_data['cache_statistics']
        report_content += f"""

## 4. ç¼“å­˜æ€§èƒ½åˆ†æ

### 4.1 ç¼“å­˜å‘½ä¸­ç‡

- **å †ç¼“å­˜å‘½ä¸­ç‡**: {cache_stats['heap_hit_ratio']:.1%}
- **ç´¢å¼•ç¼“å­˜å‘½ä¸­ç‡**: {cache_stats['index_hit_ratio']:.1%}
- **å…±äº«ç¼“å†²åŒºå¤§å°**: {cache_stats['shared_buffers']}
- **æ•´ä½“è¯„ä¼°**: {cache_stats['cache_efficiency']}

"""
        
        # è¿æ¥åˆ†æ
        conn_stats = self.report_data['connection_analysis']
        report_content += f"""

## 5. è¿æ¥åˆ†æ

### 5.1 è¿æ¥ä½¿ç”¨æƒ…å†µ

- **æ€»è¿æ¥æ•°**: {conn_stats['total_connections']}
- **æ´»è·ƒè¿æ¥**: {conn_stats['active_connections']}
- **ç©ºé—²è¿æ¥**: {conn_stats['idle_connections']}
- **ç©ºé—²äº‹åŠ¡è¿æ¥**: {conn_stats['idle_in_transaction']}
- **æœ€å¤§è¿æ¥æ•°**: {conn_stats['max_connections']}
- **è¿æ¥ä½¿ç”¨ç‡**: {conn_stats['connection_usage_ratio']:.1%}

"""
        
        # ä¼˜åŒ–å»ºè®®
        report_content += f"""

## 6. ä¼˜åŒ–å»ºè®®

### 6.1 é«˜ä¼˜å…ˆçº§å»ºè®®

"""
        
        high_priority_recs = [r for r in self.recommendations if r.priority == 'high']
        for i, rec in enumerate(high_priority_recs, 1):
            report_content += f"""
#### {i}. {rec.title}

**åˆ†ç±»**: {rec.category}  
**å½±å“**: {rec.impact}  
**å®æ–½éš¾åº¦**: {rec.implementation_effort}

**æè¿°**: {rec.description}

**å®æ–½æ­¥éª¤**:
```sql
{chr(10).join(rec.sql_commands)}
```

"""
        
        # ä¸­ä¼˜å…ˆçº§å»ºè®®
        medium_priority_recs = [r for r in self.recommendations if r.priority == 'medium']
        if medium_priority_recs:
            report_content += f"""

### 6.2 ä¸­ä¼˜å…ˆçº§å»ºè®®

"""
            for i, rec in enumerate(medium_priority_recs, 1):
                report_content += f"""
#### {i}. {rec.title}

**æè¿°**: {rec.description}  
**å½±å“**: {rec.impact}

```sql
{chr(10).join(rec.sql_commands)}
```

"""
        
        # ä½ä¼˜å…ˆçº§å»ºè®®
        low_priority_recs = [r for r in self.recommendations if r.priority == 'low']
        if low_priority_recs:
            report_content += f"""

### 6.3 ä½ä¼˜å…ˆçº§å»ºè®®

"""
            for i, rec in enumerate(low_priority_recs, 1):
                report_content += f"- **{rec.title}**: {rec.description}\n"
        
        # å®æ–½è®¡åˆ’
        report_content += f"""

## 7. å®æ–½è®¡åˆ’å»ºè®®

### 7.1 ç¬¬ä¸€é˜¶æ®µ (ç«‹å³æ‰§è¡Œ)
- æ‰§è¡Œæ‰€æœ‰é«˜ä¼˜å…ˆçº§çš„ç´¢å¼•ä¼˜åŒ–
- æ¸…ç†éœ€è¦VACUUMçš„è¡¨
- ä¿®å¤æ…¢æŸ¥è¯¢é—®é¢˜

### 7.2 ç¬¬äºŒé˜¶æ®µ (1-2å‘¨å†…)
- å®æ–½ç¼“å­˜ä¼˜åŒ–é…ç½®
- ä¼˜åŒ–è¿æ¥æ± é…ç½®
- æ‰§è¡Œä¸­ä¼˜å…ˆçº§å»ºè®®

### 7.3 ç¬¬ä¸‰é˜¶æ®µ (æŒç»­ä¼˜åŒ–)
- ç›‘æ§æ€§èƒ½æŒ‡æ ‡å˜åŒ–
- æ ¹æ®ä¸šåŠ¡å¢é•¿è°ƒæ•´é…ç½®
- å®šæœŸæ‰§è¡Œæ€§èƒ½åˆ†æ

## 8. ç›‘æ§å»ºè®®

å»ºè®®å»ºç«‹ä»¥ä¸‹ç›‘æ§æŒ‡æ ‡ï¼š

1. **æŸ¥è¯¢æ€§èƒ½ç›‘æ§**
   - å¹³å‡æŸ¥è¯¢å“åº”æ—¶é—´
   - æ…¢æŸ¥è¯¢æ•°é‡å’Œé¢‘ç‡
   - æŸ¥è¯¢ååé‡

2. **èµ„æºä½¿ç”¨ç›‘æ§**
   - ç¼“å­˜å‘½ä¸­ç‡
   - è¿æ¥ä½¿ç”¨ç‡
   - ç£ç›˜I/Oæ€§èƒ½

3. **è¡¨ç»´æŠ¤ç›‘æ§**
   - æ­»å…ƒç»„æ¯”ä¾‹
   - è¡¨è†¨èƒ€ç‡
   - VACUUM/ANALYZEæ‰§è¡Œé¢‘ç‡

---

**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–åˆ†æå™¨  
**ç‰ˆæœ¬**: 1.0  
**è”ç³»**: æ•°æ®åº“ç®¡ç†å›¢é˜Ÿ
"""
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
    
    async def _generate_json_report(self, output_path: Path):
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        logger.info(f"ç”ŸæˆJSONæŠ¥å‘Š: {output_path}")
        
        # è½¬æ¢recommendationsä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        recommendations_data = []
        for rec in self.recommendations:
            recommendations_data.append({
                'category': rec.category,
                'priority': rec.priority,
                'title': rec.title,
                'description': rec.description,
                'impact': rec.impact,
                'implementation_effort': rec.implementation_effort,
                'sql_commands': rec.sql_commands
            })
        
        json_data = {
            'generated_at': datetime.now().isoformat(),
            'database_type': 'PostgreSQL',
            'analysis_scope': 'API Permission System Tables',
            'summary': self.report_data['optimization_summary'],
            'table_statistics': self.report_data['table_statistics'],
            'index_analysis': self.report_data['index_analysis'],
            'missing_indexes': self.report_data.get('missing_indexes', []),
            'query_performance': self.report_data['query_performance'],
            'cache_statistics': self.report_data['cache_statistics'],
            'connection_analysis': self.report_data['connection_analysis'],
            'recommendations': recommendations_data
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)
    
    async def _generate_performance_charts(self, charts_dir: Path):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        logger.info(f"ç”Ÿæˆæ€§èƒ½å›¾è¡¨: {charts_dir}")
        
        charts_dir.mkdir(exist_ok=True)
        
        try:
            # 1. è¡¨å¤§å°åˆ†å¸ƒå›¾
            table_names = [t['tablename'] for t in self.report_data['table_statistics']]
            table_sizes = [t['n_live_tup'] for t in self.report_data['table_statistics']]
            
            plt.figure(figsize=(12, 6))
            plt.bar(table_names, table_sizes)
            plt.title('è¡¨è®°å½•æ•°åˆ†å¸ƒ')
            plt.xlabel('è¡¨å')
            plt.ylabel('è®°å½•æ•°')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(charts_dir / 'table_sizes.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # 2. é¡ºåºæ‰«ææ¯”ä¾‹å›¾
            seq_scan_ratios = [t['seq_scan_ratio'] for t in self.report_data['table_statistics']]
            
            plt.figure(figsize=(12, 6))
            bars = plt.bar(table_names, seq_scan_ratios)
            # æ ‡è®°é«˜é¡ºåºæ‰«æçš„è¡¨
            for i, (bar, ratio) in enumerate(zip(bars, seq_scan_ratios)):
                if ratio > 0.5:
                    bar.set_color('red')
                elif ratio > 0.3:
                    bar.set_color('orange')
                else:
                    bar.set_color('green')
            
            plt.title('è¡¨é¡ºåºæ‰«ææ¯”ä¾‹')
            plt.xlabel('è¡¨å')
            plt.ylabel('é¡ºåºæ‰«ææ¯”ä¾‹')
            plt.xticks(rotation=45)
            plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='è­¦å‘Šçº¿(50%)')
            plt.legend()
            plt.tight_layout()
            plt.savefig(charts_dir / 'seq_scan_ratios.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # 3. ç´¢å¼•ä½¿ç”¨æƒ…å†µå›¾
            if self.report_data['index_analysis']:
                index_names = [idx['indexname'][:20] + '...' if len(idx['indexname']) > 20 
                              else idx['indexname'] for idx in self.report_data['index_analysis']]
                index_scans = [idx['idx_scan'] for idx in self.report_data['index_analysis']]
                
                plt.figure(figsize=(15, 8))
                bars = plt.bar(range(len(index_names)), index_scans)
                
                # æ ‡è®°æœªä½¿ç”¨çš„ç´¢å¼•
                for i, (bar, scans) in enumerate(zip(bars, index_scans)):
                    if scans < 10:
                        bar.set_color('red')
                    elif scans < 100:
                        bar.set_color('orange')
                    else:
                        bar.set_color('green')
                
                plt.title('ç´¢å¼•ä½¿ç”¨é¢‘ç‡')
                plt.xlabel('ç´¢å¼•å')
                plt.ylabel('æ‰«ææ¬¡æ•°')
                plt.xticks(range(len(index_names)), index_names, rotation=45, ha='right')
                plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦
                plt.tight_layout()
                plt.savefig(charts_dir / 'index_usage.png', dpi=300, bbox_inches='tight')
                plt.close()
            
            # 4. ä¼˜åŒ–å»ºè®®åˆ†å¸ƒå›¾
            categories = list(self.report_data['optimization_summary']['by_category'].keys())
            counts = list(self.report_data['optimization_summary']['by_category'].values())
            
            plt.figure(figsize=(10, 8))
            plt.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)
            plt.title('ä¼˜åŒ–å»ºè®®åˆ†ç±»åˆ†å¸ƒ')
            plt.axis('equal')
            plt.tight_layout()
            plt.savefig(charts_dir / 'recommendations_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("æ€§èƒ½å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            
        except ImportError:
            logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå™¨')
    parser.add_argument('--db-url', required=True, help='æ•°æ®åº“è¿æ¥URL')
    parser.add_argument('--output-dir', default='reports', help='æŠ¥å‘Šè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    generator = DatabasePerformanceReportGenerator(args.db_url)
    
    try:
        await generator.connect()
        report_path = await generator.generate_comprehensive_report(args.output_dir)
        print(f"âœ… æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
        
    except Exception as e:
        logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    finally:
        await generator.disconnect()

if __name__ == '__main__':
    asyncio.run(main())